{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import API, Cursor, OAuthHandler, Stream\n",
    "from tweepy.streaming import StreamListener\n",
    "import creds\n",
    "import json\n",
    "import sys\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "class TwitterClient():\n",
    "    def __init__(self, twitter_user=None):\n",
    "        self.auth = TwitterAuthenicator().authenticate_twitter_app()\n",
    "        self.twitter_client = API(self.auth)\n",
    "        self.twitter_user = twitter_user\n",
    "       \n",
    "        self.tweets = []\n",
    "        \n",
    "    \n",
    "    def get_twitter_client_api(self):\n",
    "        return self.twitter_client\n",
    "\n",
    "    def get_user_timeline_tweets(self, num_tweets):\n",
    "        for tweet in Cursor(self.twitter_client.user_timeline, id=self.twitter_user).items(num_tweets):\n",
    "            self.tweets.append(tweet)\n",
    "        return self.tweets\n",
    "        \n",
    "    \n",
    "    def get_friend_list(self, num_friends):\n",
    "        friend_list = []\n",
    "        for friend in Cursor(self.twitter_client.friends).items(num_friends):\n",
    "            friend_list.append(friend)\n",
    "        return friend_list\n",
    "\n",
    "    def get_woeid_of_trending_tweets(self):\n",
    "        with open('woeid_trends.json', 'a') as tf:\n",
    "            json.dump(self.twitter_client.trends_available(), tf)\n",
    "    \n",
    "\n",
    "\n",
    "class TwitterAuthenicator():\n",
    "\n",
    "    def authenticate_twitter_app(self):\n",
    "        auth = OAuthHandler(creds.CONSUMER_API_KEY, creds.CONSUMER_SECRET)\n",
    "        auth.set_access_token(creds.ACCESS_TOKEN, creds.ACCESS_TOKEN_SECRET)\n",
    "        return auth\n",
    "\n",
    "class TwitterStreamer():\n",
    "    def __init__(self):\n",
    "        self.twiiter_authenicator = TwitterAuthenicator()\n",
    "\n",
    "    def stream_tweets(self, fname, hashtag_list):\n",
    "        listener = StdOutListener(fname)\n",
    "        auth = self.twiiter_authenicator.authenticate_twitter_app()\n",
    "        stream = Stream(auth, listener)\n",
    "        stream.filter(track=hashtag_list)\n",
    "        \n",
    "class StdOutListener(StreamListener):\n",
    "\n",
    "    def __init__(self, fname):\n",
    "        self.fname = fname\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "\n",
    "            print(data)\n",
    "            with open(fname, 'a') as tf:\n",
    "                tf.write(data)\n",
    "            return True\n",
    "        except BaseException as e:\n",
    "            print(\"error on data %s\", str(e))\n",
    "\n",
    "        return True\n",
    "\n",
    "    def on_error(self, status):\n",
    "        if status == 420:\n",
    "            # Return false if we reach rate limit\n",
    "            return False\n",
    "        print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "\n",
    "\n",
    "class TweetAnalyzer():\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.twitter_client = TwitterClient()\n",
    "        self.api = self.twitter_client.get_twitter_client_api()\n",
    "        self.name = name\n",
    "    \n",
    "    def clean_tweet(self, tweet):\n",
    "        # remove speical characters and hyperlinks\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    \n",
    "    def analyze_sentiment(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        return analysis.sentiment.polarity\n",
    "    \n",
    "    def analyze_subjectivity(self, tweet):\n",
    "        analysis = TextBlob(self.clean_tweet(tweet))\n",
    "        return analysis.sentiment.subjectivity\n",
    "    \n",
    "    def user_feed_to_database(self):\n",
    "        tweets = self.api.user_timeline(screen_name=self.name, count=200, tweet_mode=\"extended\")\n",
    "        df = self.analyse_tweets(tweets)\n",
    "        self.tweets_to_db(tweets, df)\n",
    "    \n",
    "    def tweets_to_df(self,tweets):\n",
    "        df = pd.DataFrame(data=[self.clean_tweet(tweet.full_text) for tweet in tweets], columns=['tweets'])\n",
    "        df['raw_tweet'] = np.array([tweet.full_text for tweet in tweets])\n",
    "        df['id'] = np.array([tweet.id for tweet in tweets])\n",
    "        df['date_created'] = np.array([tweet.created_at for tweet in tweets])\n",
    "        df['date_created'] = pd.to_datetime(df['date_created'])\n",
    "        df['likes'] = np.array([tweet.favorite_count for tweet in tweets])\n",
    "        df['retweet_count'] = np.array([tweet.retweet_count for tweet in tweets])\n",
    "        df['len'] = np.array([len(tweet.full_text) for tweet in tweets])\n",
    "        df['sentiment'] = np.array([self.analyze_sentiment(tweet) for tweet in df['tweets']])\n",
    "        df['subjectivity'] = np.array([self.analyze_subjectivity(tweet) for tweet in df['tweets']])\n",
    "        df['tweet_date'] = [d.date() for d in df['date_created']]\n",
    "        df['tweet_day_of_week'] = df['date_created'].dt.weekday_name\n",
    "        df['tweet_hour'] = df['date_created'].dt.hour\n",
    "        df = df[~df.tweets.str.contains(\"RT\")]\n",
    "        return df\n",
    "    \n",
    "    def tweets_to_db(self, tweets, df):\n",
    "        conn = sqlite3.connect(\"tweets.db\")\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"SELECT id from tweets\")\n",
    "        rows = cur.fetchall()\n",
    "        for idx, row in df.iterrows():\n",
    "            if(any(row['id'] in i for i in rows)):\n",
    "                cur.execute(\"\"\"UPDATE tweets SET likes = ?, retweet_count = ? WHERE id = ?\"\"\",(row['likes'], row['retweet_count'], row['id']) )\n",
    "            else:\n",
    "                cur.execute(\"\"\" INSERT OR IGNORE INTO \n",
    "                        tweets(user, user_id, raw_tweet, tweets, id, date_created, likes, retweet_count, len, sentiment, subjectivity, tweet_date, tweet_day_of_week, tweet_hour)\n",
    "                        values(?,?,?,?,?,?,?,?,?,?,?,?,?,?)\"\"\", (tweets[0].user.screen_name , tweets[0].id,row['raw_tweet'] ,row['tweets'], row['id'], row['date_created'].strftime(\"%d-%b-%Y (%H:%M:%S.%f)\"), row['likes'], row['retweet_count'],  row['len'], row['sentiment'], row['subjectivity'], row['tweet_date'], row['tweet_day_of_week'], row['tweet_hour']))\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "    \n",
    "    def analyse_tweets(self, tweets):\n",
    "        df = self.tweets_to_df(tweets)\n",
    "        df['av_len_tweet'] = np.mean(df['len'])\n",
    "        df['av_tweets_per_day'] = df.groupby('tweet_date')['tweets'].count().mean() \n",
    "        df['av_sentiment'] = np.mean(df['sentiment'])\n",
    "        df['av_subjectivity'] = np.mean(df['subjectivity'])\n",
    "        \n",
    "        max_idx = df.groupby(['tweets'])['likes'].transform(max) == df['likes'].max()\n",
    "        df['most_liked_tweet']= df[max_idx]['tweets'][:1]\n",
    "        \n",
    "        min_idx = df.groupby(['tweets'])['likes'].transform(max) == df['likes'].min()\n",
    "        df['least_liked_tweet'] = df[min_idx]['tweets'][:1]\n",
    "        \n",
    "        most_retweets_idx = df.groupby(['tweets'])['retweet_count'].transform(max) == df['retweet_count'].max()\n",
    "        df['most_retweeted_tweet'] = df[most_retweets_idx]['tweets'][:1]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write twitter users to db\n",
    "users = ['elonmusk', 'neiltyson', 'rickygervais', 'realDonaldTrump', 'TheNotoriousMMA']\n",
    "\n",
    "for user in users:\n",
    "    ta = TweetAnalyzer(user).user_feed_to_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test Twitter Analyser.\n",
    "\n",
    "\n",
    "tc = TwitterClient()\n",
    "api = tc.get_twitter_client_api()\n",
    "tweets = api.user_timeline(screen_name=\"realDonaldTrump\", count=200, tweet_mode=\"extended\")\n",
    "t_a = TweetAnalyzer(\"realDonaldTrump\")\n",
    "\n",
    "df_test = t_a.analyse_tweets(tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = df_test.groupby(['tweets'])['likes'].transform(max) == df_test['likes'].max()\n",
    "df_test['most_liked_tweet_test']= df_test[max_idx]['tweets'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand replies to tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replies=[] \n",
    "non_bmp_map = dict.fromkeys(range(0x10000, sys.maxunicode + 1), 0xfffd)  \n",
    "\n",
    "try:\n",
    "    for full_tweets in Cursor(api.user_timeline,screen_name='elonmusk',timeout=999999).items(10):\n",
    "          for tweet in Cursor(api.search,q='to:elonmusk', since_id=tweets_em[0].id, result_type='recent',timeout=999999).items(1000):\n",
    "            if hasattr(tweet, 'in_reply_to_status_id_str'):\n",
    "                  if (tweet.in_reply_to_status_id_str==full_tweets.id_str):\n",
    "                    replies.append(tweet.text)\n",
    "except BaseException as e:\n",
    "    print(\"Rate Limit: \", e)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get trending hashtags by country\n",
    "    * US: 23424977\n",
    "    * UK: 23424975\n",
    "    * Aus: 23424748\n",
    "    * Canada: 23424775\n",
    "    * NZ: 23424916"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Wordcloud for twitter user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import (WordCloud, get_single_color_func, STOPWORDS, ImageColorGenerator)\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedColorFunc(object):\n",
    "    \"\"\"Create a color function object which assigns DIFFERENT SHADES of\n",
    "       specified colors to certain words based on the color to words mapping.\n",
    "       Uses wordcloud.get_single_color_func\n",
    "       Parameters\n",
    "       ----------\n",
    "       color_to_words : dict(str -> list(str))\n",
    "         A dictionary that maps a color to the list of words.\n",
    "       default_color : str\n",
    "         Color that will be assigned to a word that's not a member\n",
    "         of any value from color_to_words.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, color_to_words, default_color):\n",
    "        self.color_func_to_words = [\n",
    "            (get_single_color_func(color), set(words))\n",
    "            for (color, words) in color_to_words.items()]\n",
    "\n",
    "        self.default_color_func = get_single_color_func(default_color)\n",
    "\n",
    "    def get_color_func(self, word):\n",
    "        \"\"\"Returns a single_color_func associated with the word\"\"\"\n",
    "        try:\n",
    "            color_func = next(\n",
    "                color_func for (color_func, words) in self.color_func_to_words\n",
    "                if word in words)\n",
    "        except StopIteration:\n",
    "            color_func = self.default_color_func\n",
    "\n",
    "        return color_func\n",
    "\n",
    "    def __call__(self, word, **kwargs):\n",
    "        return self.get_color_func(word)(word, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['tweets'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_text = [x for x in text if (TextBlob(x).sentiment.polarity > 0)]\n",
    "negative_text = [x for x in text if (TextBlob(x).sentiment.polarity < 0)]\n",
    "neutral_text = [x for x in text if (TextBlob(x).sentiment.polarity == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = np.array(Image.open('./twitter.png'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"RT\")\n",
    "stopwords.add(\"amp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(mask=twitter, width=5000, height=4000,contour_width=0.3 ,contour_color=\"#1da1f2\",background_color='white', stopwords= stopwords).generate(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_to_words = {\n",
    "    \"#1da1f2\": positive_text,\n",
    "    \"#14171a\": negative_text,\n",
    "    \"#657786\": neutral_text\n",
    "}\n",
    "\n",
    "default_color = \"#AAb8C2\"\n",
    "\n",
    "grouped_color_func = GroupedColorFunc(color_to_words, default_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.recolor(color_func=grouped_color_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (40,30))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('./data/img/'+ tweets_em[0].user.screen_name + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download('stopwords')\n",
    "import gensim\n",
    "import gensim.corpora as corpora \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.extend(['from', ' subject', 're', 'edu', 'use', 'RT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = df['tweets'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        yield(simple_preprocess(str(sent), deacc=True))\n",
    "\n",
    "data_words = list(sent_to_words(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stopwords] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_no_stopwords = remove_stopwords(data_words)\n",
    "\n",
    "tweet_bigrams = make_bigrams(tweet_no_stopwords)\n",
    "\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "data_lemmatized = lemmatization(tweet_bigrams)\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "texts = data_lemmatized\n",
    "\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus = corpus, \n",
    "                                            id2word=id2word,\n",
    "                                            num_topics = 3,\n",
    "                                            random_state = 100,\n",
    "                                            update_every = 1, \n",
    "                                            chunksize = 100,\n",
    "                                            passes = 10,\n",
    "                                            alpha = 'auto',\n",
    "                                            per_word_topics = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path =  './mallet-2.0.8/bin/mallet'\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus, num_topics=3, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ldamallet.show_topics(formatted=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start = 2, step = 3):\n",
    "    c_values= []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherence_model = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_values.append(coherence_model.get_coherence())\n",
    "        \n",
    "    return model_list, c_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, c_values = compute_coherence_values(id2word, corpus, data_lemmatized, limit=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=40; start=2; step=3;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, c_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, c_values):\n",
    "    print('Num Topics = ', m, \" C-Score = \", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[7]\n",
    "model_topics = optimal_model.show_topics(formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(optimal_model.print_topics(num_words=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
